---
layout: post
title: "Summer Project (Dark Reaction) Snippet!"
date: 2019-06-09
---

This summer I am working on the [Dark Reaction Project](https://darkreactions.haverford.edu/) with [Sorelle Friedler](http://sorelle.friedler.net/) at [Haverford College](https://www.haverford.edu/). The broad goal of this project is to use machine learning techniques to predict the outcomes of chemistry reaction. I am mainly working on the code side with another student, and there are also a couple of people working on the research side, which focuses on developing and applying interpretability techniques of machine learning model. I would like to briefly share what I have learned and done in the past two weeks:

I am mainly responsible for rewriting and extracting major functions in the recommendation engine. The end goal of the recommendation engine is to recommend the “best” reactions to chemists, which is divided into three parts:

# Generate Reaction Features.

First, we generate potential reactions based on the combination of various parameters. Some parameters are specific to the compounds that participate in the reaction (inorganic/organic molecule names and their mass). It is assumed that each reaction consists of two inorganic compounds and one organic compound, which overall forms a triple. Other parameters, which we call grid parameters, are specific to the reaction environment (PH values, temperature etc). Since there are only certain values we can choose for each parameter, the combination of these parameters would be finite. We store these potential reactions in a Dataframe, and we also need to expand the Dataframe by adding the descriptors for each compound.These descriptors describe detailed properties of a chemical compound (polarity, surface area etc.), which can be generated by a package called [ChemAxon](https://chemaxon.com/).

#2.Run trained models / Reaction Sieve**.

Then, we need to filter reactions based on the **desired descriptors**, because not all descriptors generated by ChemAxon can be passed through the ML model. But the desired descriptors depend on what ML model we choose. After filtering, we can pass these potential reactions into a trained machine learning model to predict outcomes by assigning them a score in {1, 2, 3, 4}. The success of a reaction is measured by the amount of crystal being generated.  

**3.Recommend Reactions**.

Last, we can further filter out reactions based on **ML outcome scores** and **mutual information hashing**. We might only want to keep the successful reactions predicted by the model. Then we can rank the reactions based on the [mutual information](https://en.wikipedia.org/wiki/Mutual_information), which measures the amount of addition “information” the new reactions can bring to the existing reactions. This is an interesting concept, which refers to the dependency of two random variables in probability, or the ability to infer the information of another random variable from one random variable.  If the mutual information is small, it means that the new reaction is too similar to the old reactions to give us new insights. Intuitively, mutual information is related to the euclidean distance between reactions on a vector space, where reaction represents a point based on its compounds and outcomes. We probably would further filter reactions based on a convex hull constraint set, which sets limits for combination of different parameters (e.g we can not have a reaction that runs in super high temperature with super low PH value). This is totally decided by chemists and we haven’t worked on this part yet. After all the filtering and ranking, we can get the top ‘k’ reactions and recommend them to the chemists.

The reason why we rewrote the entire part is that the existing codebase is a bit messy and not working right now. Originally, all the three parts above (Reaction generators + ML model+ recommendation system) are connected with a MySQL Database. The Database is then connected with GUI, where the chemists can enter data (PH values, compound names and their mass etc), and the entire project is dependent on *Django* framework. These overall set a lot of limitations and inconvenience for usage. That’s why we are starting to rewrite these parts independently from current Database and *Django* so that they can flexibly take any form of argument and machine learning model.

Future work in the project may include choosing different machine learning models and techniques, such as neural networks. Currently the project uses SVM in *Wekas*, which is a package written in Java. It has 89% accuracy that beats human chemists, and performs significantly better than most other packages - here is the [paper](https://www.nature.com/articles/nature17439) that describes it. But the fundamental factor that determines the performance is still unknown, which is why interpretability side comes in - try to find ways to learn about science by understanding the patterns revealed by these models.

OK. That’s it. Looking forward to the next level of progress and intellectual discovery!
